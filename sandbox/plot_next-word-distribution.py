"""
Research question:
Find the distribution over teh vocabulary (y-words) that best describes nouns.
Essentially, what words tend to follow nouns?
This can be done in a number of ways, form very simple (which is done here) to complex.
The simple approach is to assume that all examples of nouns are equally good examples of the category noun,
and that the distribution over y-words for each noun is generated by a single distribution.
this single distribution is assumed to be the one that all nouns conform to.
but this is clearly not a good assumption, because some nouns really are different,
as they occur with some y-words much more often than others.
if one's goal is precision, one would have to model each noun with a slightly different distribution.
this is not done here because the whole point is to find "the single" distribution underlying nouns,
with he incorrect, but practically useful (and necessary) assumption that all nouns are actually identical copies
of each other, and that the observed variation in co-occurrences with y-words is just due to chance.
"""

import numpy as np
import matplotlib.pyplot as plt

from preppy.latest import Prep

from startingabstract.docs import load_docs
from startingabstract import config


CORPUS_NAME = 'childes-20191112'
PROBES_NAME = 'singular-nouns-4096'

corpus_path = config.Dirs.corpora / f'{CORPUS_NAME}.txt'
train_docs, _ = load_docs(corpus_path)
prep = Prep(train_docs,
            reverse=False,
            num_types=4096,
            slide_size=3,
            batch_size=64,
            context_size=7,
            num_evaluations=10,
            )


# load probes
probes = load_probes(PROBES_NAME)
for p in probes:
    assert p in prep.store.types, p

context_size = 1  # it doesn't make sense to have larger contexts

# count co-occurrences
ct_mat, x_words, y_words_ = make_context_by_term_matrix(prep.store.tokens, context_size)
y_words = [tuple2str(yw) for yw in y_words_]  # convert tuple to str
row_ids = [y_words.index(w) for w in probes]
assert row_ids
# extract rows for test_words only - no transposition because we can only evaluate next-word predictions form model.
sliced_ct_mat = ct_mat.tocsr()[row_ids, :]
print(f'sliced_ct_mat has shape={sliced_ct_mat.shape}')
# fit probability distribution over y-words given a test-word
# assumes there is a single distribution generating all test_words
total_f = sliced_ct_mat.sum().sum()
q = [sliced_ct_mat[:, x_words.index(xw)].sum() / total_f for xw in x_words]
assert np.sum(q).item() == 1.0

# print most likely next-word predictions based on prototype
xw2p = {xw: prob for xw, prob in zip(x_words, q)}
for xw, prob in sorted(xw2p.items(), key=lambda i: i[1], reverse=True)[:100]:
    print(f'{xw:<12} prob={prob:.6f}')

fig, ax = plt.subplots()
plt.title(PROBES_NAME)
ax.plot(sorted(q))
plt.show()