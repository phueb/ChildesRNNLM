"""
Research question:
Find the distribution over teh vocabulary (y-words) that best describes nouns.
Essentially, what words tend to follow nouns?
This can be done in a number of ways, form very simple (which is done here) to complex.
The simple approach is to assume that all examples of nouns are equally good examples of the category noun,
and that the distribution over y-words for each noun is generated by a single distribution.
this single distribution is assumed to be the one that all nouns conform to.
but this is clearly not a good assumption, because some nouns really are different,
as they occur with some y-words much more often than others.
if one's goal is precision, one would have to model each noun with a slightly different distribution.
this is not done here because the whole point is to find "the single" distribution underlying nouns,
with he incorrect, but practically useful (and necessary) assumption that all nouns are actually identical copies
of each other, and that the observed variation in co-occurrences with y-words is just due to chance.
"""

import matplotlib.pyplot as plt

from preppy.latest import Prep
from categoryeval.dp import DPScorer

from startingabstract.docs import load_docs
from startingabstract import config


KEEP_PUNCTUATION = True
CORPUS_NAME = 'childes-20191112'
PROBES_NAMES = ['singular-nouns-4096', 'all-verbs-4096', 'unconditional']

corpus_path = config.Dirs.corpora / f'{CORPUS_NAME}.txt'
_train_docs, _ = load_docs(corpus_path)

if not KEEP_PUNCTUATION:
    print('Removing punctuation')
    train_docs = []
    for doc in _train_docs:
        new_doc = doc.replace(' .', '').replace(' !', '').replace(' ?', '')
        train_docs.append(new_doc)
else:
    train_docs = _train_docs

train_prep = Prep(train_docs,
                  reverse=False,
                  num_types=4096,
                  slide_size=3,
                  batch_size=64,
                  context_size=7,
                  num_evaluations=10,
                  )

dp_scorer = DPScorer(CORPUS_NAME,
                     PROBES_NAMES,
                     train_prep.store.tokens,
                     train_prep.store.types,
                     num_parts=1,
                     )


# fig
fig, ax = plt.subplots(figsize=(6, 4), dpi=config.Figs.dpi)
plt.title(f'Next-word probability distribution\npunctuation={KEEP_PUNCTUATION}')
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.tick_params(axis='both', which='both', top=False, right=False)
ax.set_xlabel('Next-word ID (frequency-sorted)')
ax.set_ylabel('Next-word Probability')
sorted_w_ids = [train_prep.store.w2id[w]
                for w in sorted(train_prep.store.types,
                                key=train_prep.store.w2f.get, reverse=True)]
# plot q (the next-word distribution conditioned on some category)
for probes_name in dp_scorer.probes_names:
    q = dp_scorer.name2q[probes_name]
    sorted_q = [q[w_id] for w_id in sorted_w_ids]
    ax.semilogx(sorted_q, label=probes_name)

plt.legend(frameon=False)
plt.show()