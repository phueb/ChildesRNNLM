# Starting-Abstract

Research code. Under active development.

## Motivation & Results

Theoretical motivation and detailed analyses of the results can be found in Philip Huebner's master's thesis, submitted in August 2019.
The thesis will become available on his [personal website](http://philhhuebner.com) in early 2020.

## Training Data

### CHILDES

The training input consists of transcribed child-directed speech from the CHILDES database.
The text used to train the RNN in my thesis is available in `data/childes-20180319.txt`. 
It was created using [CreateCHILDESCorpus](https://github.com/UIUCLearningLanguageLab/CreateCHILDESCorpus), which performs:

1) tokenization using the default tokenizer in `spacy`
2) lowercasing
3) ordering of transcripts by the age of the target child


### Wikipedia

## Installation

First, create a new virtual environment for Python 3.6. Then:

```
pip install git+https://github.com/UIUCLearningLanguageLab/StartingSmall
```

## Dependencies

To install all the dependencies, execute the following in your virtual environment: 

```bash
pip install -r requirements.txt
```

### Preppy

The text files are prepared for training using a custom Python package `Preppy`.
It is available [here](https://github.com/phueb/Preppy).
It performs no reordering of the input, and assumes instead that the lines in the text file are already in the order that they should be presented to the model.

### CategoryEval

Evaluation of semantic category knowledge requires the custom Python package `CategoryEval`.
It is available [here](https://github.com/phueb/CategoryEval).
It computes how well the model's learned representations recapitulate some human-created gold category structure.
By default, it returns the balanced accuracy, but F1 and Cohen's Kappa can be computed also.

### Ludwig

If you are a member of the UIUC Learning & Language lab, you can run the jobs in parallel on multiple machines.
This is recommended if multiple replications need to be run, or if no access to GPUs is otherwise available.

## Usage

The code is designed to run on multiple machines, at the UIUC Learning & Language Lab using a custom job submission system called [Ludwig](https://github.com/phueb/Ludwig).
If you have access to the lab's file server, you can submit jobs with `Ludwig`:

```bash
ludwig -e PATH-TO-PREPPY PATH-TO-CATEGORYEVAL
```

Alternatively, the experiment can be run locally:

```bash
ludwig-local
```

## How it works

The research relies on the concept of a category representation.
So how is a category defined? 
How do we construct a next-word probability distribution for a category of words (e.g. nouns)?
One can easily define a next-word distribution for a single word by computing relative frequencies of next-words in a corpus.
But this cannot be done for a category, because a category is not "defined by the input", but by the learner and/or researcher.
In this work, the "ideal next-word distribution" for the noun category is  computed as follows:
The simple approach is to assume that all examples of nouns are equally good examples of the category noun,
and that the distribution over y-words for each noun is generated by a single distribution.
This single distribution is assumed to be the one that all nouns conform to.
This is clearly not a good assumption, because nouns are not identical copies of each other; 
each carries different semantic content.
But this is not a problem in this case, 
because the point is to find a *single* distribution underlying all nouns.
In effect, we attribute any observed variation in the next-word distribution of nouns as random fluctuation, 
rather than as carrying information about their semantic content.
Thus, to construct a category representation, one must make decisions about what kind of information
should be considered to live at the category-level or at the word-level. 
To arrive at the category, word-level information must be discarded (treated as noise)

## Compatibility

Developed on Ubuntu 16.04 with Python 3.6
